
Transformer는 자연어 처리(NLP)에서 가장 강력한 모델 중 하나로, 번역, 문장 생성, 요약 등에 널리 쓰입니다. 그 핵심 구성은 아래와 같습니다.

> “사람이 글을 읽을 때 단어 하나하나 순서대로 읽기도 하지만,  
> 어떤 단어가 중요할지 ‘앞뒤를 동시에 보며 의미를 이해’하잖아요?”

→ 트랜스포머는 그걸 기계가 할 수 있게 만든 구조입니다.


#### 정말 간단요약
트랜스포머는 **문장에서 중요한 단어들을 동시에 비교해가며 의미를 이해하는 모델**로,  
RNN처럼 순서대로 읽지 않고도 문맥을 잘 파악할 수 있도록 만들어진 구조입니다.

| 질문                      | 답변 예시                                                                                            |
| ----------------------- | ------------------------------------------------------------------------------------------------ |
| Q. 왜 RNN보다 트랜스포머가 좋은가요? | RNN은 단어를 하나씩 처리해서 느리고, 멀리 떨어진 단어 관계 파악이 어렵습니다. 트랜스포머는 모든 단어 관계를 **병렬**로 계산하고 **장기 의존성**을 잘 잡습니다. |
| Q. 어텐션이 중요한 이유는?        | 문장에서 중요한 단어가 항상 가까이에 있는 게 아니기 때문에, 어떤 단어에 집중해야 할지 알려주는 **가중치 메커니즘**이 필요합니다. 이게 어텐션입니다.           |
| Q. ChatGPT도 트랜스포머인가요?   | 네, GPT 시리즈는 트랜스포머 구조 중에서도 **디코더만 사용하는 구조**이고, 입력 시퀀스만을 활용해 다음 단어를 예측합니다.                         |

---

## 보완된 정리 (면접 대답용)

> 트랜스포머는 문장에서 중요한 단어들을 **동시에 비교**하면서 문맥을 이해하는 구조입니다.  
> RNN처럼 단어를 순차적으로 처리하지 않고, 병렬적으로 관계를 파악하는 것이 가장 큰 특징입니다.

트랜스포머는 크게 **1. 인코더**와 **2. 디코더**로 나뉘며,

- 인코더는 입력 문장을 **이해하고 요약**하고
- 디코더는 그 이해를 바탕으로 **출력 문장을 생성**합니다.
    

### 트랜스포머 내부 구조는 다음 4가지로 구성됩니다:

1. **임베딩 (Embedding)**  
    → 자연어 단어를 숫자 벡터로 변환해, 계산 가능한 형태로 바꿉니다.
    
2. **포지셔널 인코딩 (Positional Encoding)**  
    → 트랜스포머는 단어 순서를 모르기 때문에, **위치 정보**를 벡터에 추가해줍니다.  
    예: “나는 사과를 먹었다”의 ‘나는’이 문장 맨 앞이라는 걸 알려줌.
    
3. **멀티헤드 어텐션 (Multi-Head Attention)**  
    → 어텐션은 “이 문장에서 어떤 단어가 중요한가?”를 계산하는 방식입니다.
    
    - 예를 들어 “그는 사과를 먹었다”에서 ‘먹었다’는 ‘그는’과 연결돼야 의미가 완성됩니다.
    - 어텐션은 각 단어의 Query, Key, Value 벡터를 계산해서 **중요도**를 판단하고,
    - 멀티헤드는 이를 **여러 관점에서 동시에 반복 수행**해 다양한 관계를 포착합니다.
        
4. **피드포워드 신경망 + 레지듀얼 커넥션**  
    → 어텐션 결과를 정제하고, 정보 손실을 막기 위해 **입력과 출력을 더해주는 잔차 연결(residual)** 구조로 안정성을 높입니다.
    

---

### 🔹 디코더에서 중요한 개념: **Masked Multi-Head Attention**

> 디코더는 한 번에 모든 출력을 뱉는 게 아니라 **한 단어씩** 생성합니다.  
> 그래서 뒤쪽 단어를 **미리 보면 안 되기 때문에**, **출력의 뒷부분을 가리는 마스크**를 씌웁니다.  
> 이걸 **Masked Self-Attention**이라고 합니다.

→ 즉, 디코더의 첫 번째 어텐션 블록은 **앞 단어까지만 보게 제한**되고,  
→ 두 번째 어텐션 블록에서는 **인코더의 정보와 연결**되어 출력이 완성됩니다.

| 구성 요소                    | 설명                   |
| ------------------------ | -------------------- |
| **Embedding**            | 단어를 벡터로 변환           |
| **Positional Encoding**  | 단어의 순서 정보를 제공        |
| **Multi-Head Attention** | 문맥과 관계를 여러 방향에서 파악   |
| **Residual Connection**  | 안정적인 학습을 위한 입력-출력 연결 |


![](https://i.imgur.com/7doUpqO.png)




### 1. 인코더 (Encoder)

입력 문장을 받아들여 의미 있는 벡터 표현(특징)을 추출합니다. 여러 개의 동일한 구조의 층이 반복되며, 각 층은 다음과 같은 구성요소로 이루어져 있습니다:

- **멀티헤드 어텐션 (Self-Attention)**
- **피드포워드 네트워크 (FFN)**
- **잔차 연결(Residual) + Layer Normalization**
    

### 2. 디코더 (Decoder)

인코더의 출력을 바탕으로 문장을 생성합니다. 다음 단어를 예측하며 하나씩 생성하는 구조입니다. 인코더보다 한 단계 더 복잡한 구조로 되어 있으며:

- **Self-Attention** (이전 단어들에 대한 주의)
- **Encoder-Decoder Attention** (인코더 출력에 대한 주의)
- **피드포워드 + 잔차 연결**
    

---

## 🔧 Transformer의 핵심 구성 요소

### 1. Embedding (임베딩)

- 자연어(문자/단어)를 숫자 벡터로 바꾸는 과정입니다.
- 단어마다 고유한 의미를 갖도록 고차원 벡터로 표현합니다.
- 예: `"I"` → `[0.25, -0.13, ..., 0.07]` 와 같은 텐서 형태
    

### 2. Positional Encoding (위치 인코딩)
- 트랜스포머는 **순서를 고려하지 않는 병렬 구조**입니다.
- 따라서 단어의 위치 정보를 추가로 인코딩해야 합니다.
    
- `sin`, `cos` 함수를 활용해 각 단어에 **고유한 위치 값**을 더해줍니다.
    
- 예:
    `Tom ate the meat ≠ The meat ate Tom → 각 단어의 위치를 알려줘야 의미가 바뀌지 않음!`
    

### 3. Multi-Head Attention (다중 어텐션)
- 단어 간 관계(어텐션)를 여러 시각에서 동시에 파악합니다.
- 하나의 어텐션만 사용하는 것이 아니라, **여러 개의 어텐션을 병렬적으로** 수행하여 풍부한 문맥 이해 가능
- 각각의 head는 서로 다른 의미 관계를 포착




### 4. Residual Connection (잔차 연결)
- 각 층의 입력을 출력에 더해줍니다 (Skip Connection).
- 학습이 안정되고, **기울기 소실 문제**를 줄여줌
- 구조:  
    `Layer Output = LayerNorm(x + SubLayer(x))`
    

---

