<br>

Teacher forcing
https://en.wikipedia.org/wiki/Teacher_forcing

<br>

## Attention

우리 뇌는 여러가지 사물이 있을 때, 한가지의 사물만 집중할 수 있다. 
![](https://i.imgur.com/JPBnTuT.png)
기계에게도 이것과 비슷하게 특정 사물에 집중할 수 있도록 학습시킬 수 있지 않을까?

<br>

"Attention"은 기계 학습 및 자연어 처리 분야에서 중요한 개념 중 하나로, 주로 시퀀스 데이터 처리와 관련이 있다. Attention 메커니즘은 모델이 입력 시퀀스의 각 부분에 중요도를 할당하고, 해당 부분을 조합하여 출력을 생성하는 방법을 제공한다. 

1. **개념**:
    
    - **주의 집중(Attention)**: 주의 집중은 모델이 입력 시퀀스의 각 요소(예: 단어, 토큰, 타임 스텝)에 대한 중요도를 학습하고, 출력을 생성할 때 해당 요소의 중요성을 반영하는 메커니즘이다.

    - **시퀀스-투-시퀀스 모델**: Attention은 주로 시퀀스-투-시퀀스(Seq2Seq) 모델과 함께 사용된다. Seq2Seq 모델은 입력 시퀀스를 인코딩하고, 그 인코딩된 정보를 기반으로 출력 시퀀스를 디코딩한다.

2. **작동 방식**:
    
    - **인코더-디코더 구조**: Attention은 인코더-디코더 구조에서 주로 활용됩니다. 인코더는 입력 시퀀스를 처리하고 중요 정보를 인코딩하며, 디코더는 이 정보를 사용하여 출력 시퀀스를 생성한다

    - **가중치 계산**: Attention 메커니즘은 디코더의 각 단계에서 인코더의 출력에 가중치를 할당한다. 이 가중치는 각 인코더 출력의 중요성을 나타낸다.

    - **가중 평균**: 디코더는 인코더 출력에 대한 가중 평균을 계산하여 현재 출력의 일부로 사용한다. 이것은 디코더가 현재 생성 중인 단어 또는 요소에 어떤 입력 부분에 집중해야 하는지를 결정한다.

<br>

### Seq2Seq의 구조

![](https://i.imgur.com/XDbYw9P.png)
1. **인코더 (Encoder)**:
    - 인코더는 입력 시퀀스를 처리하는 부분이다. 입력 시퀀스는 일련의 토큰 또는 단어로 구성되며, 인코더는 이를 고정 길이의 벡터로 인코딩한다.
    - 주로 순환 신경망(RNN)이나 장단기 메모리(LSTM), 그림자 어텐션(Transformer의 인코더) 등이 사용된다.
    - 인코더는 입력 시퀀스의 정보 컨텍스트 벡터(context vector)로 요약하고, 이 벡터는 디코더로 전달된다.

2. **디코더 (Decoder)**: 
    - 디코더는 인코더에서 전달된 컨텍스트 벡터를 기반으로 출력 시퀀스를 생성한다.
    - 디코더 또한 주로 RNN, LSTM, 그림자 어텐션(Transformer의 디코더) 등의 아키텍처를 사용한다.
    - 디코더는 시작 토큰을 입력으로 받아 시작하고, 이후에는 이전 출력을 입력으로 받아 다음 출력을 생성한다. 이 과정은 종료 토큰을 생성할 때까지 반복된다.

<br>

### S2S의 동작 과정


![](https://i.imgur.com/mi1V1E6.png)
![](https://i.imgur.com/s7ApTT5.png)
![](https://i.imgur.com/srYEKob.png)


"Je suis étudiant"를 "I am a student"로 번역하는 과정은 Sequence-to-Sequence (Seq2Seq) 모델에서 다음과 같이 일어난다

1. **인코딩 (Encoding)**:
   - 입력 문장 "Je suis étudiant"는 Seq2Seq 모델의 인코더(Encoder)에 입력된다. 인코더는 입력 문장의 각 단어를 순차적으로 처리하고, 각 단어에 대한 임베딩을 생성한다. 이 임베딩은 단어를 고차원 벡터로 변환한 것으로, 단어의 의미를 나타낸다.
   - 인코더는 또한 입력 문장 전체의 문맥을 요약하기 위해 숨겨진 상태(hidden state) 벡터를 생성한다. 이 상태 벡터는 디코더로 전달되며, 입력 문장의 정보를 담고 있다.

2. **디코딩 (Decoding)**:
   - 디코딩 단계에서 디코더(Decoder)는 시작 토큰("I" 또는 "SOS"와 같은)을 입력으로 받아 번역을 시작한다. 첫 번째 출력 단어를 예측하고, 이 단어는 다음 입력으로 사용된다.
   - 디코더는 인코더로부터 전달된 상태 벡터와 이전 출력 단어를 사용하여 다음 출력 단어를 예측한다. 단어 예측은 softmax 함수를 통해 다음 가능한 단어들 중 하나로 변환된다.
   - 디코더는 "I am a student"와 같은 번역 문장을 단어 단위로 순차적으로 생성한다. 디코더가 끝마침 토큰("EOS"와 같은)을 생성하거나 최대 출력 길이에 도달하면 번역 작업이 종료된다.

3. **훈련과 손실 계산 (Training and Loss Calculation)**:
   - 훈련 중에, 디코더의 출력과 정답 번역인 "I am a student"를 비교하여 손실(loss)을 계산한다. 손실은 모델의 출력과 실제 정답과의 차이를 나타낸다.
   - 역전파(backpropagation) 알고리즘을 사용하여 모델의 가중치를 업데이트하고, 모델이 번역 작업을 개선하도록 학습한다.

이러한 과정을 통해 Seq2Seq 모델은 주어진 입력 문장을 번역하는 데 사용된다. 모델은 인코더로 문맥을 인코딩하고 디코더로 문맥을 해석하여 출력 문장을 생성한다.