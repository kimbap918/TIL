
## 들어가기에 앞서

퍼셉트론(Perceptron)

인공 신경망의 가장 간단한 형태 중 하나로, 입력 벡터를 받아 출력을 생성하는 모델이다. 모든 퍼셉트론은 입력을 벡터로 받고 출력은 실수 하나가 나온다. 퍼셉트론이 입력을 벡터로 받는 이유가 뭘까?

퍼셉트론이 입력을 벡터로 처리하는 이유는 다양하다.

1. **다차원 정보 표현:** 벡터는 여러 차원의 값을 포함할 수 있다. 입력 데이터가 벡터로 표현되면 각 차원은 특정한 특성을 나타내게 되는데 이를 통해 퍼셉트론은 다양한 특성을 동시에 고려하여 학습하고 판단할 수 있다.
    
2. **행렬 및 선형 대수 연산의 유연성:** 벡터와 행렬은 선형 대수의 개념을 기반으로 하며, 퍼셉트론은 입력과 가중치 간의 선형 조합을 계산한다. 이것은 행렬 및 벡터 연산을 통해 효율적으로 수행될 수 있다.
    
3. **패턴 인식 및 분류:** 다양한 특성을 포함하는 벡터 형태의 입력을 사용하면 퍼셉트론은 복잡한 패턴이나 클래스를 인식하고 분류하는 데 더 효과적이다. 각 차원은 특정한 특성이나 패턴을 나타내므로, 벡터를 통한 입력은 다양한 정보를 효과적으로 표현할 수 있다.
    
4. **일반성:** 벡터 형태의 입력을 사용함으로써 퍼셉트론은 다양한 종류의 문제에 대응할 수 있다. 이미지, 텍스트, 오디오 등 다양한 유형의 데이터를 벡터로 표현하여 퍼셉트론을 적용할 수 있다.


시퀀스-투-시퀀스(Seq2Seq) 모델의 유형
1. **One-to-One (1:1):**
    - 단일 입력에 대해 단일 출력이 생성되는 구조. 전통적인 일반적인 신경망 모델을 나타낸다.
    - **예시:** 일반적인 피드포워드 네트워크 (예: 단일 이미지를 분류하는 모델)
2. **One-to-Many (1:N):**
    - 하나의 입력에 대해 여러 출력이 생성되는 구조. 이미지 캡션 생성과 같은 경우
    - **예시:** 이미지 캡션 생성 (하나의 이미지에 대해 여러 문장을 생성), 디코더(벡터를 문장으로, GPT같은 경우)
3. **Many-to-Many (N:N):**
    - 여러 입력에 대해 여러 출력이 생성되는 구조. 주로 시퀀스 데이터를 다른 시퀀스로 변환하는 경우에 사용된다.
    - **예시:** 기계 번역 (하나의 언어의 문장을 다른 언어의 문장으로 번역)


### RNN Gradient Vanishing

![](https://i.imgur.com/GPIH76l.png)

RNN 하나만을 사용하여 번역기를 만들면 몇가지 문제점들이 발생할 수 있다. 대표적인 예가 그래디언트 소실(Gradient Vanishing)이다.

그래디언트 소실이 발생하면, 네트워크의 초기 레이어로 갈수록 그래디언트가 지수적으로 작아지는 현상이 발생한다. 이는 역전파 과정에서 오차를 처음부터 끝까지 전파시킬 때, 초기 레이어의 가중치가 거의 업데이트되지 않아 학습이 제대로 이루어지지 않는 문제로 이어진다.

번역기로 보자면, 초기 단어의 정보가 나중에 있는 단어에 전파되지 않거나 전파되더라도 그 값이 매우 작아져서 효과적인 학습이 어려워지는 것이다.


## Seq2Seq model
* 그래디언트 ㅣ소실 문제를 완화하기 위해 LSTM (Long Short-Term Memory)과 GRU (Gated Recurrent Unit)와 같은 메모리 셀을 도입
* **attention** 매커니즘


### Attention Function
어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 **매 시점(step)** 마다, 인코더의 입력 시퀀스를 다시 참고하는 것. 이 때, 입력 시퀀스를 동일한 비중으로 참고하는 것이 아닌, 예측 단어와 **관련이 있는 입력 단어**를 더욱 치중해서 보기 때문에 **Attention**이란 단어를 사용한다.
![](https://i.imgur.com/Yi98Fz3.png)
어텐션 함수를 기호로 사용해 표현하면 아래와 같이 표현이 가능하다.

$Attention(Q,K,V)=Attentionvalue$
- Q: Query - t 시점의 **decoder cell**에서의 은닉 상태
- K: Key - 모든 시점의 **encoder cell**에서의 은닉 상태
- V: Value - 모든 시점의 **encoder cell**에서의 은닉 상태